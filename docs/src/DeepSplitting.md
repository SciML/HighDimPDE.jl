# [The `DeepSplitting` algorithm](@id deepsplitting)

```@autodocs
Modules = [HighDimPDE]
Pages   = ["DeepSplitting.jl"]
```

The `DeepSplitting` algorithm reformulates the PDE as a stochastic learning problem.

The algorithm relies on two main ideas:

- the approximation of the solution $u$ by a parametric function $\bf u^\theta$. This function is generally chosen as a (Feedforward) Neural Network, as it is [universal approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem).

- the training of $\bf u^\theta$ by simulated stochastic trajectories of particles, through the link between linear PDEs and the expected trajectory of associated Stochastic Differential Equations (SDEs), explicitly stated by the [Feynman Kac formula](https://en.wikipedia.org/wiki/Feynmanâ€“Kac_formula).

## The general idea ðŸ’¡
Consider the PDE
```math
\partial_t u(t,x) = \mu(t, x) \nabla_x u(t,x) + \frac{1}{2} \sigma^2(t, x) \Delta_x u(t,x) + f(x, u(t,x)) \tag{1}
```
with initial conditions $u(0, x) = g(x)$, where $u \colon \R^d \to \R$. 

### Local Feynman Kac formula
`DeepSplitting` solves the PDE iteratively over small time intervals by using an approximate [Feynman-Kac representation](@ref feynmankac) locally.

More specifically, considering a small time step $dt = t_{n+1} - t_n$ one has that
```math
u(t_{n+1}, X_{T - t_{n+1}}) \approx \mathbb{E} \left[ f(t, X_{T - t_{n}}, u(t_{n},X_{T - t_{n}}))(t_{n+1} - t_n) + u(t_{n}, X_{T - t_{n}}) | X_{T - t_{n+1}}\right] \tag{3}.
```
One can therefore use Monte Carlo integrations to approximate the expectations
```math
u(t_{n+1}, X_{T - t_{n+1}}) \approx \frac{1}{\text{batch\_size}}\sum_{j=1}^{\text{batch\_size}} \left[ u(t_{n}, X_{T - t_{n}}^{(j)}) + (t_{n+1} - t_n)\sum_{k=1}^{K} \big[ f(t_n, X_{T - t_{n}}^{(j)}, u(t_{n},X_{T - t_{n}}^{(j)})) \big] \right]
```


### Reformulation as a learning problem
The `DeepSplitting` algorithm approximates $u(t_{n+1}, x)$ by a parametric function ${\bf u}^\theta_n(x)$. It is advised to let this function be a neural network ${\bf u}_\theta \equiv NN_\theta$ as they are universal approximators.

For each time step $t_n$, the `DeepSplitting` algorithm 

1. Generates the particle trajectories $X^{x, (j)}$ satisfying [Eq. (2)](@ref feynmankac) over the whole interval $[0,T]$.

2. Seeks ${\bf u}_{n+1}^{\theta}$  by minimising the loss function

```math
L(\theta) = ||{\bf u}^\theta_{n+1}(X_{T - t_n}) - \left[ f(t, X_{T - t_{n-1}}, {\bf u}_{n-1}(X_{T - t_{n-1}}))(t_{n} - t_{n-1}) + {\bf u}_{n-1}(X_{T - t_{n-1}}) \right] ||
```
This way the PDE approximation problem is decomposed into a sequence of separate learning problems.
In `HighDimPDE.jl` the right parameter combination $\theta$ is found by iteratively minimizing $L$ using **stochastic gradient descent**.

!!! tip
    To solve with `DeepSplitting`, one needs to provide to `solve`
    - `dt`
    - `batch_size`
    - `maxiters`: the number of iterations for minimising the loss function
    - `abstol`: the absolute tolerance for the loss function
    - `use_cuda`: if you have a Nvidia GPU, recommended.

## Solving point-wise or on a hypercube

### Pointwise
`DeepSplitting` allows to obtain $u(t,x)$ on a single point  $x \in \Omega$ with the keyword $x$.

```julia
prob = PIDEProblem(g, f, Î¼, Ïƒ, tspan, x = x)
```

### Hypercube
Yet more generally, one wants to solve Eq. (1) on a $d$-dimensional cube $[a,b]^d$. This is offered by `HighDimPDE.jl` with the keyworkd `u_domain`.

```julia
prob = PIDEProblem(g, f, Î¼, Ïƒ, tspan, u_domain = u_domain)
```
Internally, this is handled by assigning a random variable as the initial point of the particles, i.e.
```math
X_t^\xi = \int_0^t \mu(X_s^x)ds + \int_0^t\sigma(X_s^x)dB_s + \xi,
```
where $\xi$ a random variable uniformly distributed over $[a,b]^d$. This way, the neural network is trained on the whole interval $[a,b]^d$ instead of a single point.

## Nonlocal PDEs
An extension of the `DeepSplitting` method offers to solve for non-local reaction diffusion equations of the type
```math
\partial_t u = \mu(x) \nabla_x u + \frac{1}{2} \sigma^2(x) \Delta u + \int_{\Omega}f(x,y, u(t,x), u(t,y))dy
```

The non-localness is handled by a plain vanilla Monte Carlo integration.

```math
u(t_{n+1}, X_{T - t_{n+1}}) \approx \sum_{j=1}^{\text{batch\_size}} \left[ u(t_{n}, X_{T - t_{n}}^{(j)}) + \frac{(t_{n+1} - t_n)}{K}\sum_{k=1}^{K} \big[ f(t, X_{T - t_{n}}^{(j)}, Y_{X_{T - t_{n}}^{(j)}}^{(k)}, u(t_{n},X_{T - t_{n}}^{(j)}), u(t_{n},Y_{X_{T - t_{n}}^{(j)}}^{(k)})) \big] \right]
```

!!! tip 
    In practice, if you have a non-local model you need to provide the sampling method and the number $K$ of MC integration through the keywords `mc_sample` and `K`. 
    ```julia
    alg = DeepSplitting(nn, opt = opt, mc_sample = mc_sample, K = 1)
    ```
    `mc_sample` can be whether `UniformSampling(u_domain[1], u_domain[2]))` or ` NormalSampling(Ïƒ_sampling, shifted)`.
