var documenterSearchIndex = {"docs":
[{"location":"tutorials/mlp/#MLP","page":"MLP","title":"MLP","text":"","category":"section"},{"location":"tutorials/mlp/#Local-PDE","page":"MLP","title":"Local PDE","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP","title":"MLP","text":"Let's solve the Fisher KPP PDE in dimension 10 with MLP.","category":"page"},{"location":"tutorials/mlp/","page":"MLP","title":"MLP","text":"partial_t u = u (1 - u) + frac12sigma^2Delta_xu tag1","category":"page"},{"location":"tutorials/mlp/","page":"MLP","title":"MLP","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0,0.5) # time horizon\nx0 = fill(0.,d)  # initial point\ng(x) = exp(- sum(x.^2) ) # initial condition\nŒº(x, p, t) = 0.0 # advection coefficients\nœÉ(x, p, t) = 0.1 # diffusion coefficients\nf(x, y, v_x, v_y, ‚àáv_x, ‚àáv_y, p, t) = max(0.0, v_x) * (1 -  max(0.0, v_x)) # nonlocal nonlinear part of the\nprob = PIDEProblem(g, f, Œº, œÉ, x0, tspan) # defining the problem\n\n## Definition of the algorithm\nalg = MLP() # defining the algorithm. We use the Multi Level Picard algorithm\n\n## Solving with multiple threads \nsol = solve(prob, alg, multithreading=true)","category":"page"},{"location":"tutorials/mlp/#Non-local-PDE-with-Neumann-boundary-conditions","page":"MLP","title":"Non local PDE with Neumann boundary conditions","text":"","category":"section"},{"location":"tutorials/mlp/","page":"MLP","title":"MLP","text":"Let's include in the previous equation non local competition, i.e.","category":"page"},{"location":"tutorials/mlp/","page":"MLP","title":"MLP","text":"partial_t u = u (1 - int_Omega u(ty)dy) + frac12sigma^2Delta_xu tag2","category":"page"},{"location":"tutorials/mlp/","page":"MLP","title":"MLP","text":"where Omega = -12 12^d, and let's assume Neumann Boundary condition on Omega.","category":"page"},{"location":"tutorials/mlp/","page":"MLP","title":"MLP","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0,0.5) # time horizon\nx0 = fill(0.,d)  # initial point\ng(x) = exp( -sum(x.^2) ) # initial condition\nŒº(x, p, t) = 0.0 # advection coefficients\nœÉ(x, p, t) = 0.1 # diffusion coefficients\nmc_sample = UniformSampling(fill(-5f-1, d), fill(5f-1, d))\nf(x, y, v_x, v_y, ‚àáv_x, ‚àáv_y, t) = max(0.0, v_x) * (1 -  max(0.0, v_y)) \nprob = PIDEProblem(g, f, Œº, \n                    œÉ, x0, tspan) # defining x0_sample is sufficient to implement Neumann boundary conditions\n\n## Definition of the algorithm\nalg = MLP(mc_sample = mc_sample ) \n\nsol = solve(prob, alg, multithreading=true)","category":"page"},{"location":"DeepBSDE/#deepbsde","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"","category":"section"},{"location":"DeepBSDE/","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"DeepBSDE.jl\", \"DeepBSDE_Han.jl\"]","category":"page"},{"location":"DeepBSDE/#HighDimPDE.DeepBSDE","page":"The DeepBSDE algorithm","title":"HighDimPDE.DeepBSDE","text":"DeepBSDE(u0,œÉ·µÄ‚àáu;opt=Flux.Optimise.Adam(0.1))\n\nDeepBSDE algorithm, from J. Han, A. Jentzen and Weinan E. \n\nArguments\n\nu0: a Flux.jl Chain with a d-dimensional input and a 1-dimensional output for the solytion guess.\nœÉ·µÄ‚àáu: a Flux.jl Chain for the BSDE value guess.\nopt: the optimization algorithm to be used to optimize the neural networks. Defaults to Flux.Optimise.Adam(0.1).\n\nExample\n\nBlack-Scholes-Barenblatt equation\n\nd = 30 # number of dimensions\nx0 = repeat([1.0f0, 0.5f0], div(d,2))\ntspan = (0.0f0,1.0f0)\ndt = 0.2\nm = 30 # number of trajectories (batch size)\n\nr = 0.05f0\nsigma = 0.4f0\nf(X,u,œÉ·µÄ‚àáu,p,t) = r * (u - sum(X.*œÉ·µÄ‚àáu))\ng(X) = sum(X.^2)\nŒº_f(X,p,t) = zero(X) #Vector d x 1\nœÉ_f(X,p,t) = Diagonal(sigma*X) #Matrix d x d\nprob = PIDEProblem(g, f, Œº_f, œÉ_f, x0, tspan)\n\nhls  = 10 + d #hidden layer size\nopt = Flux.Optimise.Adam(0.001)\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\nœÉ·µÄ‚àáu = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = DeepBSDE(u0, œÉ·µÄ‚àáu, opt=opt)\n\nsolve(prob, \n    pdealg, \n    EM(), \n    verbose=true, \n    maxiters=150, \n    trajectories=m, \n    sdealg=StochasticDiffEq., \n    dt=dt, \n    pabstol = 1f-6)\n\n\n\n\n\n","category":"type"},{"location":"DeepBSDE/#CommonSolve.solve-Tuple{PIDEProblem, DeepBSDE, Any}","page":"The DeepBSDE algorithm","title":"CommonSolve.solve","text":"solve(\n    prob,\n    pdealg,\n    sdealg;\n    verbose,\n    maxiters,\n    trajectories,\n    dt,\n    pabstol,\n    save_everystep,\n    limits,\n    ensemblealg,\n    trajectories_upper,\n    trajectories_lower,\n    maxiters_limits,\n    kwargs...\n)\n\n\nReturns a PIDESolution object.\n\nArguments\n\nsdealg: a SDE solver from DifferentialEquations.jl.    If not provided, the plain vanilla DeepBSDE method will be applied.   If provided, the SDE associated with the PDE problem will be solved relying on    methods from DifferentialEquations.jl, using Ensemble solves    via sdealg. Check the available sdealg on the    DifferentialEquations.jl doc.\nlimits: if true, upper and lower limits will be calculated, based on    Deep Primal-Dual algorithm for BSDEs.\nExtra keyword arguments passed to solve will be further passed to the SDE solver.\n\n\n\n\n\n","category":"method"},{"location":"DeepBSDE/#The-general-idea","page":"The DeepBSDE algorithm","title":"The general idea üí°","text":"","category":"section"},{"location":"DeepBSDE/","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"The DeepBSDE algorithm is similar in essence to the DeepSplitting algorithm, with the difference that  it uses two neural networks to approximate both the the solution and its gradient.","category":"page"},{"location":"DeepBSDE/#References","page":"The DeepBSDE algorithm","title":"References","text":"","category":"section"},{"location":"DeepBSDE/","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"Han, J., Jentzen, A., E, W., Solving high-dimensional partial differential equations using deep learning. arXiv (2018)","category":"page"},{"location":"getting_started/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"getting_started/#General-workflow","page":"Getting started","title":"General workflow","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"The general workflow for using HighDimPDE.jl is as follows:","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Define a Partial Integro-Differential Equation problem\nSelect a solver algorithm\nSolve the problem.","category":"page"},{"location":"getting_started/#Examples","page":"Getting started","title":"Examples","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's illustrate that with some examples.","category":"page"},{"location":"getting_started/#MLP","page":"Getting started","title":"MLP","text":"","category":"section"},{"location":"getting_started/#Local-PDE","page":"Getting started","title":"Local PDE","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's solve the Fisher KPP PDE in dimension 10 with MLP.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"partial_t u = u (1 - u) + frac12sigma^2Delta_xu tag1","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0., d)  # initial point\ng(x) = exp(-sum(x.^2)) # initial condition\nŒº(x, p, t) = 0.0 # advection coefficients\nœÉ(x, p, t) = 0.1 # diffusion coefficients\nf(x, y, v_x, v_y, ‚àáv_x, ‚àáv_y, p, t) = max(0.0, v_x) * (1 -  max(0.0, v_x)) # nonlocal nonlinear part of the\nprob = PIDEProblem(g, f, Œº, œÉ, x0, tspan) # defining the problem\n\n## Definition of the algorithm\nalg = MLP() # defining the algorithm. We use the Multi Level Picard algorithm\n\n## Solving with multiple threads \nsol = solve(prob, alg, multithreading = true)","category":"page"},{"location":"getting_started/#Non-local-PDE-with-Neumann-boundary-conditions","page":"Getting started","title":"Non-local PDE with Neumann boundary conditions","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's include in the previous equation non-local competition, i.e.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"partial_t u = u (1 - int_Omega u(ty)dy) + frac12sigma^2Delta_xu tag2","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"where Omega = -12 12^d, and let's assume Neumann Boundary condition on Omega.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0,0.5) # time horizon\nx0 = fill(0.,d)  # initial point\ng(x) = exp( -sum(x.^2) ) # initial condition\nŒº(x, p, t) = 0.0 # advection coefficients\nœÉ(x, p, t) = 0.1 # diffusion coefficients\nmc_sample = UniformSampling(fill(-5f-1, d), fill(5f-1, d))\nf(x, y, v_x, v_y, ‚àáv_x, ‚àáv_y, p, t) = max(0.0, v_x) * (1 -  max(0.0, v_y)) \nprob = PIDEProblem(g, f, Œº, œÉ, x0, tspan) # defining x0_sample is sufficient to implement Neumann boundary conditions\n\n## Definition of the algorithm\nalg = MLP(mc_sample = mc_sample)\n\nsol = solve(prob, alg, multithreading = true)","category":"page"},{"location":"getting_started/#DeepSplitting","page":"Getting started","title":"DeepSplitting","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's solve the previous equation with DeepSplitting.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\nusing Flux # needed to define the neural network\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0f0, d)  # initial point\ng(x) = exp.(-sum(x.^2, dims=1)) # initial condition\nŒº(x, p, t) = 0.0f0 # advection coefficients\nœÉ(x, p, t) = 0.1f0 # diffusion coefficients\nx0_sample = UniformSampling(fill(-5f-1, d), fill(5f-1, d))\nf(x, y, v_x, v_y, ‚àáv_x, ‚àáv_y, p, t) = v_x .* (1f0 .- v_y)\nprob = PIDEProblem(g, f, Œº, œÉ, x0, tspan,\n                   x0_sample = x0_sample)\n\n## Definition of the neural network to use\nhls = d + 50 #hidden layer size\n\nnn = Flux.Chain(Dense(d, hls, tanh),\n                Dense(hls, hls, tanh),\n                Dense(hls, 1)) # neural network used by the scheme\n\nopt = ADAM(1e-2)\n\n## Definition of the algorithm\nalg = DeepSplitting(nn,\n                    opt = opt,\n                    mc_sample = x0_sample)\n\nsol = solve(prob, \n            alg, \n            0.1, \n            verbose = true, \n            abstol = 2e-3,\n            maxiters = 1000,\n            batch_size = 1000)","category":"page"},{"location":"getting_started/#Solving-on-the-GPU","page":"Getting started","title":"Solving on the GPU","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"DeepSplitting can run on the GPU for (much) improved performance. To do so, just set use_cuda = true.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"sol = solve(prob, \n            alg, \n            0.1, \n            verbose = true, \n            abstol = 2e-3,\n            maxiters = 1000,\n            batch_size = 1000,\n            use_cuda=true)","category":"page"},{"location":"Feynman_Kac/#feynmankac","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"","category":"section"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"The Feynman Kac formula is generally stated for terminal condition problems (see e.g. Wikipedia), where","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"partial_t u(tx) + mu(x) nabla_x u(tx) + frac12 sigma^2(x) Delta_x u(tx) + f(x u(tx))  = 0 tag1","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"with terminal condition u(T x) = g(x), and u colon R^d to R. ","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"In this case, the FK formula states that for all t in (0T) it holds that","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"u(t x) = int_t^T mathbbE left f(X^x_s-t u(s X^x_s-t))ds right + mathbbE left u(0 X^x_T-t) right tag2","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"where ","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"X_t^x = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + x","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"and B_t is a Brownian motion.","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"(Image: Brownian motion - Wikipedia)","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Intuitively, this formula is motivated by the fact that the density of Brownian particles (motion) satisfies the diffusion equation.","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"The equivalence between the average trajectory of particles and PDEs given by the Feynman-Kac formula allows overcoming the curse of dimensionality that standard numerical methods suffer from, because the expectations can be approximated Monte Carlo integrations, which approximation error decreases as 1sqrtN and is therefore not dependent on the dimensions. On the other hand, the computational complexity of traditional deterministic techniques grows exponentially in the number of dimensions. ","category":"page"},{"location":"Feynman_Kac/#Forward-non-linear-Feynman-Kac","page":"Feynman Kac formula","title":"Forward non-linear Feynman-Kac","text":"","category":"section"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"How to transform previous equation to an initial value problem?","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Define v(tau x) = u(T-tau x). Observe that v(0x) = u(Tx). Further, observe that by the chain rule","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"beginaligned\npartial_tau v(tau x) = partial_tau u(T-taux)\n                        = (partial_tau (T-tau)) partial_t u(T-taux)\n                        = -partial_t u(T-tau x)\nendaligned","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"From Eq. (1) we get that ","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"- partial_t u(T - taux) = mu(x) nabla_x u(T - taux) + frac12 sigma^2(x) Delta_x u(T - taux) + f(x u(T - taux))","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Replacing  u(T-tau x) by v(tau x) we get that v satisfies","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"partial_tau v(tau x) = mu(x) nabla_x v(taux) + frac12 sigma^2(x) Delta_x v(taux) + f(x v(taux)) ","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"and from Eq. (2) we obtain","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"v(tau x) = int_T-tau^T mathbbE left f(X^x_s- T + tau v(s X^x_s-T + tau))ds right + mathbbE left v(0 X^x_tau) right","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"By using the substitution rule with tau to tau -T (shifting by T) and tau to - tau (inversing), and finally inversing the integral bound we get that ","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"beginaligned\nv(tau x) = int_-tau^0 mathbbE left f(X^x_s + tau v(s + T X^x_s + tau))ds right + mathbbE left v(0 X^x_tau) right\n            = - int_tau^0 mathbbE left f(X^x_tau - s v(T-s X^x_tau - s))ds right + mathbbE left v(0 X^x_tau) right\n            = int_0^tau mathbbE left f(X^x_tau - s v(T-s X^x_tau - s))ds right + mathbbE left v(0 X^x_tau) right\nendaligned","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"This leads to the ","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"info: Non-linear Feynman Kac for initial value problems\nConsider the PDEpartial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx))with initial conditions u(0 x) = g(x), where u colon R^d to R.  Thenu(t x) = int_0^t mathbbE left f(X^x_t - s u(T-s X^x_t - s))ds right + mathbbE left u(0 X^x_t) right tag3with X_t^x = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + x","category":"page"},{"location":"tutorials/deepbsde/#DeepBSDE","page":"DeepBSDE","title":"DeepBSDE","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"<!‚Äì TODO: revise the code, which should not work right here ‚Äì>","category":"page"},{"location":"tutorials/deepbsde/#Solving-a-100-dimensional-Hamilton-Jacobi-Bellman-Equation","page":"DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"First, here's a fully working code for the solution of a 100-dimensional Hamilton-Jacobi-Bellman equation that takes a few minutes on a laptop:","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"using NeuralPDE\nusing Flux, OptimizationOptimisers\nusing DifferentialEquations\nusing LinearAlgebra\nd = 100 # number of dimensions\nX0 = fill(0.0f0, d) # initial value of stochastic control process\ntspan = (0.0f0, 1.0f0)\nŒª = 1.0f0\n\ng(X) = log(0.5f0 + 0.5f0 * sum(X.^2))\nf(X,u,œÉ·µÄ‚àáu,p,t) = -Œª * sum(œÉ·µÄ‚àáu.^2)\nŒº_f(X,p,t) = zero(X)  # Vector d x 1 Œª\nœÉ_f(X,p,t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) # Matrix d x d\nprob = PIDEProblem(g, f, Œº_f, œÉ_f, X0, tspan)\nhls = 10 + d # hidden layer size\nopt = Optimisers.Adam(0.01)  # optimizer\n# sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d, hls, relu),\n                Dense(hls, hls, relu),\n                Dense(hls, 1))\n# sub-neural network approximating the spatial gradients at time point\nœÉ·µÄ‚àáu = Flux.Chain(Dense(d + 1, hls, relu),\n                  Dense(hls, hls, relu),\n                  Dense(hls, hls, relu),\n                  Dense(hls, d))\npdealg = NNPDENS(u0, œÉ·µÄ‚àáu, opt=opt)\n@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,\n                            alg=EM(), dt=1.2, pabstol=1f-2)","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"Now, let's explain the details!","category":"page"},{"location":"tutorials/deepbsde/#Hamilton-Jacobi-Bellman-Equation","page":"DeepBSDE","title":"Hamilton-Jacobi-Bellman Equation","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"The Hamilton-Jacobi-Bellman equation is the solution to a stochastic optimal control problem.","category":"page"},{"location":"tutorials/deepbsde/#Symbolic-Solution","page":"DeepBSDE","title":"Symbolic Solution","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"Here, we choose to solve the classical Linear Quadratic Gaussian (LQG) control problem of 100 dimensions, which is governed by the SDE","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"d X_t = 2 sqrtlambda c_t dt + sqrt2dW_t","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"where c_t is a control process. The solution to the optimal control is given by a PDE of the form:","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"partial_t u(tx) + Delta_x u + lambda  Nabla u (tx)^2 = 0 ","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"with terminating condition g(x) = log(12 + 12 x^2)).","category":"page"},{"location":"tutorials/deepbsde/#Solving-LQG-Problem-with-Neural-Net","page":"DeepBSDE","title":"Solving LQG Problem with Neural Net","text":"","category":"section"},{"location":"tutorials/deepbsde/#Define-the-Problem","page":"DeepBSDE","title":"Define the Problem","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"To get the solution above using the PIDEProblem, we write:","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"d = 100 # number of dimensions\nX0 = fill(0.0f0,d) # initial value of stochastic control process\ntspan = (0.0f0, 1.0f0)\nŒª = 1.0f0\n\ng(X) = log(0.5f0 + 0.5f0*sum(X.^2))\nf(X,u,œÉ·µÄ‚àáu,p,t) = -Œª*sum(œÉ·µÄ‚àáu.^2)\nŒº_f(X,p,t) = zero(X)  #Vector d x 1 Œª\nœÉ_f(X,p,t) = Diagonal(sqrt(2.0f0)*ones(Float32,d)) #Matrix d x d\nprob = PIDEProblem(g, f, Œº_f, œÉ_f, X0, tspan)","category":"page"},{"location":"tutorials/deepbsde/#Define-the-Solver-Algorithm","page":"DeepBSDE","title":"Define the Solver Algorithm","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d dimensional -> 1 dimensional chain, while œÉ·µÄ‚àáu needs to be d+1 dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"hls = 10 + d #hidden layer size\nopt = Flux.Optimise.Adam(0.01)  #optimizer\n#sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\n# sub-neural network approximating the spatial gradients at time point\nœÉ·µÄ‚àáu = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = NNPDENS(u0, œÉ·µÄ‚àáu, opt=opt)","category":"page"},{"location":"tutorials/deepbsde/#Solving-with-Neural-Nets","page":"DeepBSDE","title":"Solving with Neural Nets","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,\n                            alg=EM(), dt=0.2, pabstol = 1f-2)\n","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"Here we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 100 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-2.","category":"page"},{"location":"tutorials/deepbsde/#Solving-the-100-dimensional-Black-Scholes-Barenblatt-Equation","page":"DeepBSDE","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"Black Scholes equation is a model for stock option price. In 1973, Black and Scholes transformed their formula on option pricing and corporate liabilities into a PDE model, which is widely used in financing engineering for computing the option price over time. [1.] In this example, we will solve a Black-Scholes-Barenblatt equation of 100 dimensions. The Black-Scholes-Barenblatt equation is a nonlinear extension to the Black-Scholes equation, which models uncertain volatility and interest rates derived from the Black-Scholes equation. This model results in a nonlinear PDE whose dimension is the number of assets in the portfolio.","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"To solve it using the PIDEProblem, we write:","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"d = 100 # number of dimensions\nX0 = repeat([1.0f0, 0.5f0], div(d,2)) # initial value of stochastic state\ntspan = (0.0f0,1.0f0)\nr = 0.05f0\nsigma = 0.4f0\nf(X,u,œÉ·µÄ‚àáu,p,t) = r * (u - sum(X.*œÉ·µÄ‚àáu))\ng(X) = sum(X.^2)\nŒº_f(X,p,t) = zero(X) #Vector d x 1\nœÉ_f(X,p,t) = Diagonal(sigma*X) #Matrix d x d\nprob = PIDEProblem(g, f, Œº_f, œÉ_f, X0, tspan)","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d-dimensional -> 1-dimensional chain, while œÉ·µÄ‚àáu needs to be d+1-dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"hls  = 10 + d #hide layer size\nopt = Flux.Optimise.Adam(0.001)\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\nœÉ·µÄ‚àáu = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = NNPDENS(u0, œÉ·µÄ‚àáu, opt=opt)","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"And now we solve the PDE. Here, we say we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 150 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-6.","category":"page"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"ans = solve(prob, pdealg, verbose=true, maxiters=150, trajectories=100,\n                            alg=EM(), dt=0.2, pabstol = 1f-6)","category":"page"},{"location":"tutorials/deepbsde/#References","page":"DeepBSDE","title":"References","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"DeepBSDE","title":"DeepBSDE","text":"Shinde, A. S., and K. C. Takale. \"Study of Black-Scholes model and its applications.\" Procedia Engineering 38 (2012): 270-279.","category":"page"},{"location":"DeepSplitting/#deepsplitting","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"DeepSplitting.jl\"]","category":"page"},{"location":"DeepSplitting/#HighDimPDE.DeepSplitting","page":"The DeepSplitting algorithm","title":"HighDimPDE.DeepSplitting","text":"DeepSplitting(nn, K=1, opt = Flux.Optimise.Adam(0.01), Œªs = nothing, mc_sample =  NoSampling())\n\nDeep splitting algorithm.\n\nArguments\n\nnn: a Flux.Chain, or more generally a functor.\nK: the number of Monte Carlo integrations.\nopt: optimizer to be used. By default, Flux.Optimise.Adam(0.01).\nŒªs: the learning rates, used sequentially. Defaults to a single value taken from opt.\nmc_sample::MCSampling : sampling method for Monte Carlo integrations of the non-local term. Can be UniformSampling(a,b), NormalSampling(œÉ_sampling, shifted), or NoSampling (by default).\n\nExample\n\nhls = d + 50 # hidden layer size\nd = 10 # size of the sample\n\n# Neural network used by the scheme\nnn = Flux.Chain(Dense(d, hls, tanh),\n                Dense(hls,hls,tanh),\n                Dense(hls, 1, x->x^2))\n\nalg = DeepSplitting(nn, K=10, opt = Flux.Optimise.Adam(), Œªs = [5e-3,1e-3],\n                    mc_sample = UniformSampling(zeros(d), ones(d)) )\n\n\n\n\n\n","category":"type"},{"location":"DeepSplitting/#CommonSolve.solve-Tuple{PIDEProblem, DeepSplitting, Any}","page":"The DeepSplitting algorithm","title":"CommonSolve.solve","text":"solve(\n    prob,\n    alg,\n    dt;\n    batch_size,\n    abstol,\n    verbose,\n    maxiters,\n    use_cuda,\n    cuda_device,\n    verbose_rate\n)\n\n\nReturns a PIDESolution object.\n\nArguments\n\nmaxiters: number of iterations per time step. Can be a tuple, where maxiters[1] is used for the training of the neural network used in the first time step (which can be long) and maxiters[2] is used for the rest of the time steps.\nbatch_size : the batch size.\nabstol : threshold for the objective function under which the training is stopped.\nverbose : print training information.\nverbose_rate : rate for printing training information (every verbose_rate iterations).\nuse_cuda : set to true to use CUDA.\ncuda_device : integer, to set the CUDA device used in the training, if use_cuda == true.\n\n\n\n\n\n","category":"method"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The DeepSplitting algorithm reformulates the PDE as a stochastic learning problem.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The algorithm relies on two main ideas:","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The approximation of the solution u by a parametric function bf u^theta. This function is generally chosen as a (Feedforward) Neural Network, as it is a universal approximator.\nThe training of bf u^theta by simulated stochastic trajectories of particles, through the link between linear PDEs and the expected trajectory of associated Stochastic Differential Equations (SDEs), explicitly stated by the Feynman Kac formula.","category":"page"},{"location":"DeepSplitting/#The-general-idea","page":"The DeepSplitting algorithm","title":"The general idea üí°","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Consider the PDE","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"partial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx)) tag1","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"with initial conditions u(0 x) = g(x), where u colon R^d to R. ","category":"page"},{"location":"DeepSplitting/#Local-Feynman-Kac-formula","page":"The DeepSplitting algorithm","title":"Local Feynman Kac formula","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting solves the PDE iteratively over small time intervals by using an approximate Feynman-Kac representation locally.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"More specifically, considering a small time step dt = t_n+1 - t_n one has that","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx mathbbE left f(t X_T - t_n u(t_nX_T - t_n))(t_n+1 - t_n) + u(t_n X_T - t_n)  X_T - t_n+1right tag3","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"One can therefore use Monte Carlo integrations to approximate the expectations","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx frac1textbatch_sizesum_j=1^textbatch_size left u(t_n X_T - t_n^(j)) + (t_n+1 - t_n)sum_k=1^K big f(t_n X_T - t_n^(j) u(t_nX_T - t_n^(j))) big right","category":"page"},{"location":"DeepSplitting/#Reformulation-as-a-learning-problem","page":"The DeepSplitting algorithm","title":"Reformulation as a learning problem","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The DeepSplitting algorithm approximates u(t_n+1 x) by a parametric function bf u^theta_n(x). It is advised to let this function be a neural network bf u_theta equiv NN_theta as they are universal approximators.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"For each time step t_n, the DeepSplitting algorithm ","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Generates the particle trajectories X^x (j) satisfying Eq. (2) over the whole interval 0T.\nSeeks bf u_n+1^theta  by minimizing the loss function","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"L(theta) = bf u^theta_n+1(X_T - t_n) - left f(t X_T - t_n-1 bf u_n-1(X_T - t_n-1))(t_n - t_n-1) + bf u_n-1(X_T - t_n-1) right ","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"This way, the PDE approximation problem is decomposed into a sequence of separate learning problems. In HighDimPDE.jl the right parameter combination theta is found by iteratively minimizing L using stochastic gradient descent.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"tip: Tip\nTo solve with DeepSplitting, one needs to provide to solvedt\nbatch_size\nmaxiters: the number of iterations for minimizing the loss function\nabstol: the absolute tolerance for the loss function\nuse_cuda: if you have a Nvidia GPU, recommended.","category":"page"},{"location":"DeepSplitting/#Solving-point-wise-or-on-a-hypercube","page":"The DeepSplitting algorithm","title":"Solving point-wise or on a hypercube","text":"","category":"section"},{"location":"DeepSplitting/#Pointwise","page":"The DeepSplitting algorithm","title":"Pointwise","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting allows obtaining u(tx) on a single point  x in Omega with the keyword x.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"prob = PIDEProblem(g, f, Œº, œÉ, x, tspan)","category":"page"},{"location":"DeepSplitting/#Hypercube","page":"The DeepSplitting algorithm","title":"Hypercube","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Yet more generally, one wants to solve Eq. (1) on a d-dimensional cube ab^d. This is offered by HighDimPDE.jl with the keyword x0_sample.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"prob = PIDEProblem(g, f, Œº, œÉ, x, tspan, x0_sample = x0_sample)","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Internally, this is handled by assigning a random variable as the initial point of the particles, i.e.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"X_t^xi = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + xi","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"where xi a random variable uniformly distributed over ab^d. This way, the neural network is trained on the whole interval ab^d instead of a single point.","category":"page"},{"location":"DeepSplitting/#Non-local-PDEs","page":"The DeepSplitting algorithm","title":"Non-local PDEs","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting can solve for non-local reaction diffusion equations of the type","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"partial_t u = mu(x) nabla_x u + frac12 sigma^2(x) Delta u + int_Omegaf(xy u(tx) u(ty))dy","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The non-localness is handled by a Monte Carlo integration.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx sum_j=1^textbatch_size left u(t_n X_T - t_n^(j)) + frac(t_n+1 - t_n)Ksum_k=1^K big f(t X_T - t_n^(j) Y_X_T - t_n^(j)^(k) u(t_nX_T - t_n^(j)) u(t_nY_X_T - t_n^(j)^(k))) big right","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"tip: Tip\nIn practice, if you have a non-local model, you need to provide the sampling method and the number K of MC integration through the keywords mc_sample and K. alg = DeepSplitting(nn, opt = opt, mc_sample = mc_sample, K = 1)mc_sample can be whether UniformSampling(a, b) or NormalSampling(œÉ_sampling, shifted).","category":"page"},{"location":"DeepSplitting/#References","page":"The DeepSplitting algorithm","title":"References","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Boussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. arXiv (2022)\nBeck, C., Becker, S., Cheridito, P., Jentzen, A., Neufeld, A., Deep splitting method for parabolic PDEs. arXiv (2019)","category":"page"},{"location":"MLP/#mlp","page":"The MLP algorithm","title":"The MLP algorithm","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"MLP.jl\"]","category":"page"},{"location":"MLP/#HighDimPDE.MLP","page":"The MLP algorithm","title":"HighDimPDE.MLP","text":"Multi level Picard algorithm.\n\nArguments\n\nL: number of Picard iterations (Level),\nM: number of Monte Carlo integrations (at each level l, M^(L-l)integrations),\nK: number of Monte Carlo integrations for the non-local term\nmc_sample::MCSampling : sampling method for Monte Carlo integrations of the non local term.\n\nCan be UniformSampling(a,b), NormalSampling(œÉ_sampling), or NoSampling (by default).\n\n\n\n\n\n","category":"type"},{"location":"MLP/#CommonSolve.solve-Tuple{PIDEProblem, MLP}","page":"The MLP algorithm","title":"CommonSolve.solve","text":"solve(prob, alg; multithreading, verbose)\n\n\nReturns a PIDESolution object.\n\nArguments\n\nmultithreading : if true, distributes the job over all the threads available.\nverbose: print information over the iterations.\n\n\n\n\n\n","category":"method"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP, for Multi-Level Picard iterations, reformulates the PDE problem as a fixed point equation through the Feynman Kac formula. ","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"It relies on Picard iterations to find the fixed point, \nreducing the complexity of the numerical approximation of the time integral through a multilevel Monte Carlo approach.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm overcomes the curse of dimensionality, with a computational complexity that grows polynomially in the number of dimension (see M. Hutzenthaler et al. 2020).","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"warning: `MLP` can only approximate the solution on a single point\nMLP only works for PIDEProblem with x0_sample = NoSampling(). If you want to solve over an entire domain, you definitely want to check the DeepSplitting algorithm.","category":"page"},{"location":"MLP/#The-general-idea","page":"The MLP algorithm","title":"The general idea üí°","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Consider the PDE","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"partial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx)) tag1","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"with initial conditions u(0 x) = g(x), where u colon R^d to R. ","category":"page"},{"location":"MLP/#Picard-Iterations","page":"The MLP algorithm","title":"Picard Iterations","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm observes that the Feynman Kac formula can be viewed as a fixed point equation, i.e. u = phi(u). Introducing a sequence (u_k) defined as u_0 = g and ","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"u_l+1 = phi(u_l)","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"the Banach fixed-point theorem ensures that the sequence converges to the true solution u. Such a technique is known as Picard iterations.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The time integral term is evaluated by a Monte-Carlo integration","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"u_L  = frac1Msum_i^M left f(X^x(i)_t - s_(l i) u_L-1(T-s_i X^x( i)_t - s_(l i))) + u(0 X^x(i)_t - s_(l i)) right","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"But the MLP uses an extra trick to lower the computational cost of the iteration. ","category":"page"},{"location":"MLP/#Telescope-sum","page":"The MLP algorithm","title":"Telescope sum","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm uses a telescope sum ","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = phi(u_L-1) = phi(u_L-1) - phi(u_L-2) + phi(u_L-2) - phi(u_L-3) + dots \n= sum_l=1^L-1 phi(u_l-1) - phi(u_l-2)\nendaligned","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"As l grows, the term phi(u_l-1) - phi(u_l-2) becomes smaller, and thus demands more calculations. The MLP algorithm uses this fact by evaluating the integral term at level l with M^L-l samples.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"tip: Tip\nL corresponds to the level of the approximation, i.e. u approx u_L\nM characterizes the number of samples for the Monte Carlo approximation of the time integral","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Overall, MLP can be summarized by the following formula","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = sum_l=1^L-1 frac1M^L-lsum_i^M^L-l left f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i))) + mathbf1_N(l) f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i)))right\n\nqquad + frac1M^Lsum_i^M^L u(0 X^x(l i)_t)\nendaligned","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Note that the superscripts (l i) indicate the independence of the random variables X across levels.","category":"page"},{"location":"MLP/#Non-local-PDEs","page":"The MLP algorithm","title":"Non-local PDEs","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"MLP can solve for non-local reaction diffusion equations of the type","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"partial_t u = mu(t x) nabla_x u(t x) + frac12 sigma^2(t x) Delta u(t x) + int_Omegaf(x y u(tx) u(ty))dy","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The non-localness is handled by a Monte Carlo integration.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = sum_l=1^L-1 frac1M^L-lsum_i=1^M^L-l frac1Ksum_j=1^K  bigg f(X^x(l i)_t - s_(l i) Z^(lj) u(T-s_(l i) X^x(l i)_t - s_(l i)) u(T-s_li Z^(lj))) + \nqquad \nmathbf1_N(l) f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i)))bigg + frac1M^Lsum_i^M^L u(0 X^x(l i)_t)\nendaligned","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"tip: Tip\nIn practice, if you have a non-local model, you need to provide the sampling method and the number K of MC integration through the keywords mc_sample and K. K characterizes the number of samples for the Monte Carlo approximation of the last term.\nmc_sample characterizes the distribution of the Z variables","category":"page"},{"location":"MLP/#References","page":"The MLP algorithm","title":"References","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Boussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. arXiv (2022)\nBecker, S., Braunwarth, R., Hutzenthaler, M., Jentzen, A., von Wurstemberger, P., Numerical simulations for full history recursive multilevel Picard approximations for systems of high-dimensional partial differential equations. arXiv (2020)","category":"page"},{"location":"#HighDimPDE.jl","page":"Home","title":"HighDimPDE.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"HighDimPDE.jl is a Julia package to solve Highly Dimensional non-linear, non-local PDEs of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\n    (partial_t u)(tx) = int_Omega fbig(txbf x u(tx)u(tbf x) ( nabla_x u )(tx )( nabla_x u )(tbf x ) big)  dbf x \n     quad + biglangle mu(tx) ( nabla_x u )( tx ) bigrangle + tfrac12 textTrace big(sigma(tx)  sigma(tx) ^* ( textHess_x u)(t x ) big)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where u colon 0T times Omega to R, Omega subseteq R^d is subject to initial and boundary conditions, and where d is large.","category":"page"},{"location":"","page":"Home","title":"Home","text":"HighDimPDE.jl implements solver algorithms that break down the curse of dimensionality, including","category":"page"},{"location":"","page":"Home","title":"Home","text":"the Deep Splitting scheme\nthe Multi-Level Picard iterations scheme\nthe Deep BSDE scheme (@ref deepbsde).","category":"page"},{"location":"","page":"Home","title":"Home","text":"To make the most out of HighDimPDE.jl, we advise to first have a look at the ","category":"page"},{"location":"","page":"Home","title":"Home","text":"documentation on the Feynman Kac formula,","category":"page"},{"location":"","page":"Home","title":"Home","text":"as all solver algorithms heavily rely on it.","category":"page"},{"location":"#Algorithm-overview","page":"Home","title":"Algorithm overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Features DeepSplitting MLP DeepBSDE\nTime discretization free ‚ùå ‚úÖ ‚ùå\nMesh-free ‚úÖ ‚úÖ ‚úÖ\nSingle point x in R^d approximation ‚úÖ ‚úÖ ‚úÖ\nd-dimensional cube ab^d approximation ‚úÖ ‚ùå ‚úîÔ∏è\nGPU ‚úÖ ‚ùå ‚úÖ\nGradient non-linearities ‚úîÔ∏è ‚ùå ‚úÖ","category":"page"},{"location":"","page":"Home","title":"Home","text":"‚úîÔ∏è : will be supported in the future","category":"page"},{"location":"#Reproducibility","page":"Home","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"tutorials/deepsplitting/#DeepSplitting","page":"DeepSplitting","title":"DeepSplitting","text":"","category":"section"},{"location":"tutorials/deepsplitting/","page":"DeepSplitting","title":"DeepSplitting","text":"Consider the Fisher-KPP equation with non-local competition","category":"page"},{"location":"tutorials/deepsplitting/","page":"DeepSplitting","title":"DeepSplitting","text":"partial_t u = u (1 - int_Omega u(ty)dy) + frac12sigma^2Delta_xu tag1","category":"page"},{"location":"tutorials/deepsplitting/","page":"DeepSplitting","title":"DeepSplitting","text":"where Omega = -12 12^d, and let's assume Neumann Boundary condition on Omega.","category":"page"},{"location":"tutorials/deepsplitting/","page":"DeepSplitting","title":"DeepSplitting","text":"Let's solve Eq. (1) with the DeepSplitting solver. ","category":"page"},{"location":"tutorials/deepsplitting/","page":"DeepSplitting","title":"DeepSplitting","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0f0,d)  # initial point\ng(x) = exp.(- sum(x.^2, dims=1) ) # initial condition\nŒº(x, p, t) = 0.0f0 # advection coefficients\nœÉ(x, p, t) = 0.1f0 # diffusion coefficients\nx0_sample = UniformSampling(fill(-5f-1, d), fill(5f-1, d))\nf(x, y, v_x, v_y, ‚àáv_x, ‚àáv_y, p, t) = v_x .* (1f0 .- v_y)\nprob = PIDEProblem(g, f, Œº, \n                    œÉ, x0, tspan, \n                    x0_sample = x0_sample)\n\n## Definition of the neural network to use\nusing Flux # needed to define the neural network\n\nhls = d + 50 #hidden layer size\n\nnn = Flux.Chain(Dense(d, hls, tanh),\n        Dense(hls, hls, tanh),\n        Dense(hls, 1)) # neural network used by the scheme\n\nopt = ADAM(1e-2)\n\n## Definition of the algorithm\nalg = DeepSplitting(nn,\n                    opt = opt,\n                    mc_sample = x0_sample)\n\nsol = solve(prob, \n            alg, \n            0.1, \n            verbose = true, \n            abstol = 2e-3,\n            maxiters = 1000,\n            batch_size = 1000)","category":"page"},{"location":"tutorials/deepsplitting/#Solving-on-the-GPU","page":"DeepSplitting","title":"Solving on the GPU","text":"","category":"section"},{"location":"tutorials/deepsplitting/","page":"DeepSplitting","title":"DeepSplitting","text":"DeepSplitting can run on the GPU for (much) improved performance. To do so, just set use_cuda = true.","category":"page"},{"location":"tutorials/deepsplitting/","page":"DeepSplitting","title":"DeepSplitting","text":"sol = solve(prob, \n            alg, \n            0.1, \n            verbose = true, \n            abstol = 2e-3,\n            maxiters = 1000,\n            batch_size = 1000,\n            use_cuda=true)","category":"page"}]
}
